{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/home/files/cnsi-sed2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0, EfficientNetB7\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, ReLU\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def resize_images(images, size, method='bilinear', align_corners=False):\n",
    "    \"\"\" See https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/image/resize_images .\n",
    "    Args\n",
    "        method: The method used for interpolation. One of ('bilinear', 'nearest', 'bicubic', 'area').\n",
    "    \"\"\"\n",
    "    methods = {\n",
    "        'bilinear': tf.image.ResizeMethod.BILINEAR,\n",
    "        'nearest' : tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "        'bicubic' : tf.image.ResizeMethod.BICUBIC,\n",
    "        'area'    : tf.image.ResizeMethod.AREA,\n",
    "    }\n",
    "    return tf.compat.v1.image.resize_images(images, size, methods[method], align_corners)\n",
    "\n",
    "class UpsampleLike(tf.keras.layers.Layer):\n",
    "    \"\"\" Keras layer for upsampling a Tensor to be the same shape as another Tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        source, target = inputs\n",
    "        target_shape = tf.keras.backend.shape(target)\n",
    "        if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "            source = tf.transpose(source, (0, 2, 3, 1))\n",
    "            output = resize_images(source, (target_shape[2], target_shape[3]), method='nearest')\n",
    "            output = tf.transpose(output, (0, 3, 1, 2))\n",
    "            return output\n",
    "        else:\n",
    "            return resize_images(source, (target_shape[1], target_shape[2]), method='nearest')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "            return (input_shape[0][0], input_shape[0][1]) + input_shape[1][2:4]\n",
    "        else:\n",
    "            return (input_shape[0][0],) + input_shape[1][1:3] + (input_shape[0][-1],)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The number of anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import anchor\n",
    "\n",
    "anchor_param = {\"ratios\": [0.5, 1, 2],\n",
    "                \"scales\": [1.0, 1.25,1.58],\n",
    "                           \"fm_sizes\": [64, 32, 16, 8, 4],\n",
    "                           \"image_size\": 512} #anchor parameters\n",
    "default_boxes = anchor.generate_retina_boxes(anchor_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient + RetinaNet build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(512,512,3))\n",
    "backbone = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(backbone, to_file='model.png', show_shapes=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = backbone.get_layer(\"block1a_project_bn\").output\n",
    "print(\"C1\", C1.shape)\n",
    "C2 = backbone.get_layer(\"block2b_add\").output\n",
    "print(\"C2\", C2.shape)\n",
    "C3 = backbone.get_layer(\"block3b_add\").output\n",
    "print(\"C3\", C3.shape)\n",
    "C4 = backbone.get_layer(\"block4c_add\").output\n",
    "print(\"C4\", C4.shape)\n",
    "C5 = backbone.get_layer(\"block5c_add\").output\n",
    "print(\"C5\", C5.shape)\n",
    "C6 = backbone.get_layer(\"block6d_add\").output\n",
    "print(\"C6\", C6.shape)\n",
    "C7 = backbone.get_layer(\"top_activation\").output\n",
    "print(\"C7\", C7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C3.shape, C5.shape, C7.shape) # They will be P3,P4,P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPN(features, channel_depth):\n",
    "    P3,P4,P5 = features   \n",
    "    P5 = Conv2D(channel_depth,kernel_size=1,strides=1,padding='same')(P5)\n",
    "    P5_upsampled = UpsampleLike(name='P5_upsampled')([P5, P4])\n",
    "    \n",
    "    P4 = Conv2D(channel_depth,kernel_size=1,strides=1,padding='same')(P4)\n",
    "    P4 = tf.add(P4, P5_upsampled)\n",
    "    P4_upsampled = UpsampleLike(name='P4_upsampled')([P4, P3])\n",
    "    \n",
    "    P3 = Conv2D(channel_depth,kernel_size=1,strides=1,padding='same')(P3)\n",
    "    P3 = tf.add(P3, P4_upsampled)\n",
    "    \n",
    "    P6 = Conv2D(channel_depth,kernel_size=3,strides=2,padding='same')(P5)\n",
    "    P7 = Conv2D(channel_depth,kernel_size=3,strides=2,padding='same')(P6)\n",
    "    P7 = ReLU()(P7)\n",
    "    return [P3,P4,P5,P6,P7] \n",
    "\n",
    "[P3,P4,P5,P6,P7] = FPN([C3,C5,C7], 256)\n",
    "print(P3.shape,P4.shape,P5.shape,P6.shape,P7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_depth = 256\n",
    "num_classes = 4\n",
    "k_anchors = 15\n",
    "\n",
    "def classification_head(_input, channel_depth, fmap_size, num_classes, num_anchors):\n",
    "    x = Conv2D(channel_depth, kernel_size=3, padding='same')(_input)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(channel_depth, kernel_size=3, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(channel_depth, kernel_size=3, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(channel_depth, kernel_size=3, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(num_classes*num_anchors, kernel_size=3, padding='same')(x)\n",
    "    x = tf.keras.activations.sigmoid(x)\n",
    "    x = tf.reshape(x, [-1, fmap_size*fmap_size*num_anchors,num_classes])\n",
    "    return x\n",
    "    \n",
    "#conf header\n",
    "confs = []\n",
    "fmap_size = 64\n",
    "conf = classification_head(P3, channel_depth, fmap_size, num_classes, k_anchors)\n",
    "confs.append(conf)\n",
    "    \n",
    "fmap_size = 32\n",
    "conf = classification_head(P4, channel_depth, fmap_size, num_classes, k_anchors)\n",
    "confs.append(conf)\n",
    "\n",
    "fmap_size = 16\n",
    "conf = classification_head(P5, channel_depth, fmap_size, num_classes, k_anchors)\n",
    "confs.append(conf)\n",
    "\n",
    "fmap_size = 8\n",
    "conf = classification_head(P6, channel_depth, fmap_size, num_classes, k_anchors)\n",
    "confs.append(conf)\n",
    "\n",
    "fmap_size = 4\n",
    "conf = classification_head(P7, channel_depth, fmap_size, num_classes, k_anchors)\n",
    "confs.append(conf)\n",
    "\n",
    "confs = tf.concat(confs, axis=-2)\n",
    "print(confs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_depth = 256\n",
    "coords = 4\n",
    "k_anchors = 15\n",
    "\n",
    "def bbox_regression_head(_input, channel_depth, fmap_size, coords, num_anchors):\n",
    "    x = Conv2D(channel_depth, kernel_size=3, padding='same')(_input)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(channel_depth, kernel_size=3, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(channel_depth, kernel_size=3, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(channel_depth, kernel_size=3, padding='same')(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(coords*num_anchors, kernel_size=3, padding='same')(x)\n",
    "    x = tf.reshape(x, [-1, fmap_size*fmap_size*num_anchors,coords])\n",
    "    return x\n",
    "    \n",
    "#conf header\n",
    "locs = []\n",
    "fmap_size = 64\n",
    "loc = bbox_regression_head(P3, channel_depth, fmap_size, coords, k_anchors)\n",
    "locs.append(loc)\n",
    "    \n",
    "fmap_size = 32\n",
    "loc = bbox_regression_head(P4, channel_depth, fmap_size, coords, k_anchors)\n",
    "locs.append(loc)\n",
    "\n",
    "fmap_size = 16\n",
    "loc = bbox_regression_head(P5, channel_depth, fmap_size, coords, k_anchors)\n",
    "locs.append(loc)\n",
    "\n",
    "fmap_size = 8\n",
    "loc = bbox_regression_head(P6, channel_depth, fmap_size, coords, k_anchors)\n",
    "locs.append(loc)\n",
    "\n",
    "fmap_size = 4\n",
    "loc = bbox_regression_head(P7, channel_depth, fmap_size, coords, k_anchors)\n",
    "locs.append(loc)\n",
    "\n",
    "locs = tf.concat(locs, axis=-2)\n",
    "print(locs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Retinanet = tf.keras.Model(inputs=input_layer, outputs=[confs, locs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Retinanet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(Retinanet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Retinanet.save(\"Retina-efficient-512-imagenet.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
