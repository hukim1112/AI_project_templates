{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import session_config\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\" # 사용 gpu 선택\n",
    "session_config.setup_gpus(True, 0.25) # gpu 메모리 사용량 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b417ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c65b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# load your model already built at phase 1.\n",
    "model = tf.keras.models.load_model(\"models/cifar_classifier.h5\")\n",
    "\n",
    "# load your dataset with tf.data\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(train_x, train_y), (val_x, val_y) = cifar10.load_data()\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(64)\n",
    "train_ds = train_ds.shuffle(buffer_size = 20000, reshuffle_each_iteration=False).batch(64).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304150d",
   "metadata": {},
   "source": [
    "# Regular training phase\n",
    "\n",
    "Phase 2 정규학습에서는 모델의 대략적인 성능을 확인하면서, 하이퍼파라미터 튜닝 이전에 주요한 요인들을 사전에 결정한다. \n",
    "\n",
    "정규학습에서 결정하는 주요한 하이퍼파라미터는 다음과 같다. \n",
    "\n",
    "- learning rate = [1E-3, 1E-4, 1E-5]\n",
    "- learning rate scheduler = [constant, piecewise decay, linear decay, cosine decay restart]\n",
    "- Initial parameter = [scratch, imagenet]\n",
    "\n",
    "리스트에 포함된 파라미터값에 의해 총 3x4x2 = 24가지 경우의 수에 대해 실험할 수 있다. 각 실험에 대해 Early stopping 지점까지 학습하고 validation 정확도를 측정한다. 우리는 주요한 하이퍼파라미터를 Grid search 방식으로 사전에 결정한다. 이런 방식은 차후 하이퍼파라미터 튜닝에서 더 낮은 차원의 파라미터 공간에 대한 탐색을 수행할 수 있다. 이는 하이퍼파라미터 최적화에 대한 약간의 타협을 통해 효율적인 파라미터 튜닝을 가능하게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3dcc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "#하이퍼파라미터 설정\n",
    "HP_LR = hp.HParam('lr', hp.Discrete([1E-3, 1E-4, 1E-5]))\n",
    "HP_LR_scheduler = hp.HParam('lr scheduler', hp.Discrete(['constant', 'piecewide decay',\n",
    "                                                        'linear decay', 'cosine decay restart']))\n",
    "HP_INIT = hp.HParam('init', hp.Discrete(['scratch', 'imagenet']))\n",
    "\n",
    "hparams = [HP_LR, HP_LR_scheduler, HP_INIT]\n",
    "\n",
    "#log 파일 생성\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_LR, HP_LR_scheduler, HP_INIT],\n",
    "    metrics=[hp.Metric('accuracy', display_name='Accuracy')],\n",
    "  )\n",
    "\n",
    "def write_log(run_dir, hparams, accuracy):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    tf.summary.scalar('accuracy', accuracy, step=1)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return tf.keras.models.load_model(\"models/cifar_classifier.h5\")\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, images, labels, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        images = preprocessing(images)\n",
    "        preds = model(images)\n",
    "        loss = loss_fn(labels, preds)\n",
    "        l2_loss = [tf.nn.l2_loss(t) for t in model.trainable_variables]\n",
    "        l2_loss = l2_reg * tf.math.reduce_sum(l2_loss)\n",
    "        total_loss += l2_loss\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return total_loss, loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, images, labels):\n",
    "    images = preprocessing(images)\n",
    "    preds = model(images)\n",
    "    loss = loss_fn(labels, preds)\n",
    "    return loss\n",
    "\n",
    "def train_test(build_model, session_num, hparams):\n",
    "    model = build_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(hparams[\"lr\"])\n",
    "    for epoch in range(EPOCHS):\n",
    "        for x,y in train_ds:\n",
    "            loss, train_loss, train_acc = train_step(model, x, y, optimizer)\n",
    "        for x,y in val_ds:\n",
    "            loss, test_loss, test_acc = test_step(model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309bf28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "for init in HP_INIT.domain.values:\n",
    "  for lr_scheduler in HP_LR_scheduler.domain.values:\n",
    "    for lr in HP_LR.domain.values:\n",
    "        hparams = {\n",
    "          HP_LR.name.name: lr,\n",
    "          HP_LR_scheduler.name: lr_scheduler,\n",
    "          HP_INIT.name: init}\n",
    "        accuracy = train_test(hparams)\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        write_log('logs/hparam_tuning/' + run_name, hparams, accuracy)\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776b3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e32b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
